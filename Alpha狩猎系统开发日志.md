# Alpha狩猎系统开发日志

---

## V4.0 开发规划：四维决策系统升级

*最后更新: 2025-07-23*

在V3.0版本成功完成云端基础部署后，我们正式启动V4.0的深度增强开发。此版本的目标是将AlphaHunter从一个基于技术面分析的“机会发现引擎”，革命性地升级为一个融合**技术面、基本面、资金面、消息面**的“**四维综合决策辅助系统**”，并引入初步的策略管理能力，使其真正成为强大、可靠的投资决策辅助工具。

---

### V3.0 部署总结与状态更新

*最后更新: 2025-07-23*

经过一系列的本地联调与云端部署实践，AlphaHunter V3.0版本已在阿里云服务器上成功部署并投入运行。后端服务通过Gunicorn与Systemd实现了持久化，前端通过Nginx进行托管与代理，前后端WebSocket通信正常。

在部署过程中，我们解决了包括但不限于SSH连接、国内外网络代理、NPM依赖安装、Nginx权限以及Gunicorn进程超时在内的多个关键技术难题。

**当前已知问题与展望**:
1.  **API Key验证**: 部署时使用的智谱AI API Key已失效，需更新为有效Key才能使AI分析功能正常工作。 (已通过`systemd`的`EnvironmentFile`机制解决配置加载问题)
2.  **Gunicorn超时**: 在调试模式下，由于对多个机会进行串行AI分析，导致任务总时长超过Gunicorn默认的30秒超时阈值，引发工作进程被重启。 (已通过减少调试数据量临时解决，V4将通过异步任务或增加超时时间彻底解决)

这些部署过程中的实践，验证了V3.0工程化架构的有效性，也为V4.0的深度功能增强打下了坚实的基础。

---

#### **V3.0-Enhanced 执行大纲 - 最终状态**

*   **第一阶段：云端基建与环境部署** - `[已完成]`
*   **第二阶段：核心引擎升级** - `[已完成]`
*   **第三阶段：服务化与部署** - `[已完成]`
*   **第四阶段：软件工程实践增强** - `[未开始]` (其核心任务，如日志、健壮性、测试等，已并入V4.0开发规划，并提升优先级)

---

## V3.0 开发与调试日志

*最后更新: 2024-07-30*

在V3.0的开发和本地测试阶段，我们遇到并解决了几个对项目成功至关重要的技术问题。记录这些问题旨在为未来的开发和维护提供宝贵的经验。

### **核心问题1：后端服务因`ImportError`启动失败**

*   **现象**: 在本地启动后端服务时，程序因 `ImportError: cannot import name 'get_market_data'` 错误而直接崩溃。
*   **根源分析**: 在将 `data_provider.py` 模块从模拟数据升级为Tushare真实数据源的过程中，核心函数 `get_market_data()` 被重构并重命名为 `get_realtime_market_data()`。然而，上游调用该函数的模块 `scanner.py` 未能同步更新其导入和调用语句。
*   **解决方案**: 审查并修正了 `scanner.py` 中的代码，将对旧函数 `get_market_data` 的引用全部更新为新函数 `get_realtime_market_data`，解决了模块间的依赖矛盾。
*   **教训**: 在进行代码重构，特别是对核心API（函数名、参数等）进行修改时，必须进行全局性的影响分析，并确保所有调用方都已同步更新。

### **核心问题2：前端服务因“错误的执行目录”启动失败**

*   **现象**: 在本地启动前端服务时，终端反复出现 `npm error enoent Could not read package.json` 的错误，导致 `http://localhost:5173` 地址无法访问。
*   **根源分析**: `npm run dev` 命令是在项目的根目录 (`/AlphaHunter`) 下执行的。`npm` 命令需要读取当前目录下的 `package.json` 文件来了解项目脚本和依赖，但该文件实际位于 `frontend/` 子目录中。在错误的位置执行命令，导致 `npm` 因找不到配置文件而失败。
*   **解决方案**: 严格遵守操作规程，在启动前端服务前，**必须先使用 `cd frontend` 命令进入前端项目目录**，然后再执行 `npm run dev`。
*   **教训**: 终端命令的执行上下文（当前所在目录）至关重要。对于包含多个子模块（如前端和后端）的项目，执行特定模块的命令前，必须确保终端位于正确的子目录中。

---

## V3.0-Enhanced：专业工程化升级 - 执行大纲

*最后更新: 2024-07-29*

基于V2.0原型的成功，我们正式启动V3.0增强版开发。此版本旨在将系统从一个成功的原型，转变为一个部署在云端、由真实数据和强大AI驱动的、具备专业工程实践的实战系统。

---

### **第一阶段：云端基建与环境部署 (Infrastructure & Deployment)**

*   **目标**: 在国内云服务器上，为项目后端和前端搭建一个稳定、可靠的运行环境。
*   **步骤**:
    1.  **服务器准备**: 采购并初始化一台国内云服务器（如阿里云ECS），配置好安全组，放行SSH(22)、HTTP(80)、HTTPS(443)等必要端口。
    2.  **环境安装**: 在服务器上安装 Nginx、Git 以及 Python 3.10+。
    3.  **项目部署**: 从代码仓库克隆 `AlphaHunter` 项目到服务器，并为其创建和配置独立的Python虚拟环境。
    4.  **依赖安装**: 在虚拟环境中，根据 `requirements.txt` 安装所有后端依赖。

### **第二阶段：核心引擎升级 (Core Logic Upgrade)**

*   **目标**: 将系统的数据源和分析引擎，从模拟数据替换为真实的API服务。
*   **步骤**:
    1.  **配置中心化**:
        *   创建 `config.py` 或 `.env` 配置文件。
        *   将所有敏感信息和可变参数（如 Tushare 和 智谱AI 的API Tokens, 扫描参数等）移入该文件，代码中通过读取配置来使用。
    2.  **数据源对接 (Tushare Pro)**:
        *   重构 `data_provider.py`。
        *   实现 `get_realtime_data_from_tushare()` 函数，调用Tushare Pro的接口获取A股市场的实时行情数据。
        *   用真实数据获取逻辑，彻底替换原有的模拟数据生成代码。
    3.  **AI分析引擎对接 (智谱AI GLM-4)**:
        *   重构 `ai_analyzer.py`。
        *   实现 `get_analysis_from_glm4()` 函数，该函数负责：
            *   根据 `scanner.py` 筛选出的机会数据，精心构造高质量的Prompt。
            *   调用智谱AI GLM-4的API接口。
            *   解析并验证返回的结构化JSON数据。
        *   用真实的AI分析逻辑，替换原有的模拟分析代码。

### **第三阶段：服务化与部署 (Production Deployment)**

*   **目标**: 将前后端应用以生产级的标准部署并对外提供服务。
*   **步骤**:
    1.  **后端服务化 (Gunicorn + Systemd)**:
        *   使用 `Gunicorn`作为WSGI服务器来运行我们的FastAPI应用。
        *   编写一个`systemd`服务单元文件，将Gunicorn进程作为系统服务来管理，实现开机自启、异常重启等功能。
    2.  **前端打包**:
        *   修改前端代码 (`HomeView.vue`) 中的WebSocket连接地址，使其指向我们云服务器的公网域名或IP。
        *   在`frontend`目录下执行 `npm run build`，生成用于生产部署的静态文件。
    3.  **统一接入层 (Nginx)**:
        *   配置Nginx作为反向代理和Web服务器。
        *   将来自公网的HTTP/HTTPS请求，代理到后端的Gunicorn服务。
        *   配置Nginx以正确处理WebSocket的升级请求 (`/ws/dashboard`)。
        *   配置Nginx直接托管前端打包生成的静态文件 (`frontend/dist`目录)。
        *   (推荐) 配置SSL证书（如使用Let's Encrypt），启用HTTPS。

### **第四阶段：软件工程实践增强 (Engineering Excellence)**

*   **目标**: 引入专业的软件工程实践，提升系统的健壮性、可维护性和可扩展性。
*   **步骤**:
    1.  **日志系统**: 集成 `loguru`，在后端代码的关键路径（如服务启动、API调用、数据处理、错误捕获）中添加结构化日志。
    2.  **健壮性强化**: 全面审查对外部API（Tushare, GLM-4）的调用，增加完备的`try-except`异常捕获、超时处理和重试机制。
    3.  **单元测试**: 使用 `pytest`，为项目中的核心纯函数（如数据清洗、指标计算、配置读取等模块）编写第一批单元测试，确保代码质量。

---
## V2.0 开发日志与决策

### 最终部署方案规划 - V3.0增强版蓝图

在最终部署方案确定后，我们对项目的未来进行了全面、深入的规划。
1.  **大模型选型**:
    *   我们对比了国内外多个主流大模型（通义千问、混元、文心一言、GLM-4、豆包、DeepSeek、Gemini、Claude）。
    *   最终确定以**智谱AI的GLM-4**作为核心分析引擎，因为它性能强大、不绑定云厂商、性价比高。同时，将**通义千问**等作为未来增强分析维度的辅助模型。

2.  **真实数据API选型**:
    *   为了让系统实战化，我们决定用真实数据API替换模拟数据。
    *   确定以**Tushare Pro**作为主数据源，**BaoStock**作为备用数据源，因为它们对个人开发者友好，性价比极高。

3.  **软件工程最佳实践**:
    *   您提出了将早期讨论的优化点（配置化、日志、错误处理、单元测试）融入项目的重要性。
    *   我们将这些工程实践与功能开发相结合，形成了一份最终的、完整的 **《V3.0-Enhanced：专业工程化升级蓝图》**。

### 重大战略转型与部署

在原型成功后，我们进行了一系列重大的战略调整和部署工作。
1.  **战略转型：聚焦中国A股**
    *   **决策**: 您基于实际情况（如与IBKR的资金往来问题），决定将项目目标市场从美股全面转向中国A股。
    *   **执行**: 我们迅速调整了代码，`data_provider.py`被修改为生成模拟A股数据，`ai_analyzer.py`也被更新为产出更符合A股市场语境的分析报告。

2.  **首次云端部署尝试 (Render + Vercel)**
    *   **后端部署**: 我们为项目添加了`gunicorn`和`Procfile`，成功将Python后端部署到了**Render.com**，并获得了公网URL。
    *   **前端部署**: 我们更新了前端代码中的WebSocket地址指向云端后端，并尝试将前端部署到**Vercel**。
    *   **部署Debug**:
        *   遇到了**404 NOT_FOUND**错误。原因是Vercel默认从根目录构建，而我们的项目在`frontend/`子目录。我们通过修改Vercel项目设置中的**Root Directory**为`frontend`来解决。
        *   解决了404后，又遇到了页面卡在“正在连接...”的问题。原因是Render的免费服务会**休眠**。我们通过先访问后端URL“唤醒”服务，再刷新前端页面的方式解决。

3.  **战略再转型：All in 国内云**
    *   **决策**: 考虑到Render的休眠策略、国内访问速度以及未来接入国内大模型的需求，您最终决定放弃海外云平台，将前后端全部部署到**国内云服务器**（如阿里云/腾讯云）。

### 前端驾驶舱开发与联调 (V2.0 阶段三)

我们成功地构建了前端原型并完成了与后端的对接。
1.  **技术选型与搭建**: 我们决定采用Vue.js作为前端框架，并在`frontend`目录下成功初始化了一个包含TypeScript, Vue Router, Pinia的现代化Vue.js项目。
2.  **前后端联调与Debug**:
    *   我们在`HomeView.vue`中实现了WebSocket连接逻辑，并成功连接到后端。
    *   遇到了**数据无法显示**的问题。通过对后端终端日志和浏览器开发者控制台的联合排查，我们定位并解决了两个核心Bug：
        1.  **后端崩溃**: 后端代码中`if opportunities:`的写法对于Pandas DataFrame是错误的，导致后台任务崩溃。我们将其修正为`if not opportunities_df.empty:`。
        2.  **前端渲染错误**: 前端收到的JSON数据键名为`change_pct`，而模板中误用为`change_percent`，导致`TypeError`。我们修正了模板中的变量名。
    *   经过修复，我们成功在前端页面上看到了由后端实时推送的数据表格。

### 后端原型与服务化 (V2.0 阶段一 & 二)

我们严格按照计划，成功完成了前两个阶段的核心任务：
1.  **项目初始化**: 创建了`src`目录及核心模块文件（`data_provider.py`, `scanner.py`, `ai_analyzer.py`, `main.py`），并搭建了Python虚拟环境。
2.  **后端核心引擎 (阶段一)**: 我们成功实现了在命令行产生模拟数据、根据规则扫描机会、并生成模拟AI分析的完整流程。
3.  **Web服务化 (阶段二)**:
    *   我们使用FastAPI和Uvicorn将后端逻辑改造成了一个Web服务。
    *   创建了`/ws/dashboard`的WebSocket端点，并实现了一个`ConnectionManager`来管理客户端连接。
    *   将核心扫描分析逻辑封装成一个每10秒运行一次的后台任务，通过WebSocket向所有连接的前端广播结果。
4.  **关键问题解决**: 在此期间，我们解决了多个技术难题，包括：
    *   **中文乱码问题**: 通过使用FastAPI底层的`Response`对象并强制指定`Content-Type`头为`application/json; charset=utf-8`彻底解决。
    *   **服务健壮性问题**: 通过在`ConnectionManager`中增加异常捕获和存在性检查，优雅地处理了客户端突然掉线时可能导致的后台任务崩溃问题，使服务达到了生产级的健-壮性。

### 项目启动与规划 (V2.0 蓝图)

项目的核心目标是构建一个自动化交易机会发现系统，并明确了V2.0的四阶段开发蓝图：
1.  **后端核心引擎**: 在本地用纯Python命令行跑通核心逻辑。
2.  **Web服务化**: 将第一阶段的逻辑通过FastAPI和WebSocket提供实时数据接口。
3.  **前端驾驶舱**: 开发Vue.js或React前端，连接后端服务，实现数据可视化。
4.  **部署与通知**: 部署前后端服务，并集成移动端推送。